{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c018277",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center> <h1> BUSINESS CASES WITH DATA SCIENCE </h1> </center> <br>\n",
    "<center> Business Case 5: Cryptocurrency Data Visualization </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f27037",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"group\">\n",
    "    \n",
    "### Group\n",
    "    \n",
    "</a>\n",
    "\n",
    "- Celso Christiano Endres Neto\t\t    |   m20200739 <br>\n",
    "- Gabriel Felipe Martins de Souza   \t|   m20210598 <br>\n",
    "- Luiz Humberto Polaro Vizeu\t\t    |   m20210554 <br>\n",
    "- Rogerio Domingos Paulo\t        \t|   m20210597 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446757c",
   "metadata": {},
   "source": [
    "**Table of Contents** <br>\n",
    "* [1.0 Import](#import)\n",
    "    * [1.1 Import Libs](#libs)\n",
    "* [2.0 Time Series (Box-Jenkins)](#timeseries)\n",
    "* [3.0 Time Series Data Preparation and Preprocessing](#data_prep_ts)\n",
    "* [4.0 Time Series Model and Assessment](#ts_modeling)\n",
    "* [5.0 Machine Learning](#ml)\n",
    "* [6.0 ML Data Preparation and Preprocessing](#data_prep_ml)\n",
    "* [7.0 Machine Learning Model and Assessment](#ml_modeling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314052e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"import\">\n",
    "    \n",
    "# 1.0 Import\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf238f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"libs\">\n",
    "    \n",
    "## 1.1 Import Libs\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157b75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from math import ceil, pi, sqrt\n",
    "import os\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#!pip install  holidays\n",
    "import holidays\n",
    "import itertools\n",
    "\n",
    "#dataviz\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import graphviz\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#algorithms for data preparation and preprocessing\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "# !pip install ta\n",
    "import ta\n",
    "from ta import add_all_ta_features\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "#Modeling and Assessment\n",
    "from sklearn import datasets, linear_model\n",
    "#!pip install XGBoost\n",
    "from sklearn.metrics import mean_squared_error as MSE, r2_score, mean_absolute_percentage_error as MAPE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#Time Series and Modeling\n",
    "#!pip install pmdarima\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "#importing stock data from Yahoo Finance\n",
    "#!pip install yahoo-finance\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa7e4a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"timeseries\">\n",
    "    \n",
    "# 2.0 Time Series (Box_Jenkins)\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4ced1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"data_prep_ts\">\n",
    "    \n",
    "# 3.0 Time Series Data Preparation and Preprocessing\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddab1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of crptocurrencies as ticker arguments\n",
    "cryptocurrencies = ['ADA-USD', 'ATOM-USD', 'AVAX-USD', 'AXS-USD', 'BTC-USD', 'ETH-USD',\n",
    "                     'LINK-USD', 'LUNA1-USD', 'MATIC-USD', 'SOL-USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce51d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed\n"
     ]
    }
   ],
   "source": [
    "##importing data from yahoo finance lib\n",
    "data = yf.download(cryptocurrencies, period = '365d', interval = '1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a429c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing each indicator in separately dataframe\n",
    "df_open = data['Open'].reset_index()\n",
    "\n",
    "df_close = data['Close'].reset_index()\n",
    "\n",
    "df_adj_close = data['Adj Close'].reset_index()\n",
    "\n",
    "df_high = data['High'].reset_index()\n",
    "\n",
    "df_low = data['Low'].reset_index()\n",
    "\n",
    "df_volume = data['Volume'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdcfe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list with all the currencies\n",
    "list_of_currencys = df_volume.iloc[:,1:].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55047dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "    \n",
    "    df[currency] = pd.DataFrame()\n",
    "\n",
    "    #retrieving open price\n",
    "    df1 = df_open[['Date',currency]].copy()\n",
    "    #filtering only non-null records\n",
    "    df1 = df1[~df1[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to open, which means the Open price for the currency\n",
    "    df1.rename(columns={currency: \"open\"}, inplace=True)\n",
    "\n",
    "    #retrieving close price\n",
    "    df2 = df_close[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df2 = df2[~df2[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to close, which means the Open price for the currency\n",
    "    df2.rename(columns={currency: \"close\"}, inplace=True)\n",
    "\n",
    "    #retrieving adj_close price\n",
    "    df3 = df_adj_close[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df3 = df3[~df3[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to adj_close, which means the adj_close price for the currency\n",
    "    df3.rename(columns={currency: \"adj_close\"}, inplace=True)\n",
    "\n",
    "    #retrieving highest price\n",
    "    df4 = df_high[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df4 = df4[~df4[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to high, which means the highest price for the currency\n",
    "    df4.rename(columns={currency: \"high\"}, inplace=True)\n",
    "\n",
    "    #retrieving lowest price\n",
    "    df5 = df_low[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df5 = df5[~df5[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to df5, which means the lowest price for the currency\n",
    "    df5.rename(columns={currency: \"low\"}, inplace=True)\n",
    "\n",
    "    #retrieving Volume\n",
    "    df6 = df_volume[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df6 = df6[~df6[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to Volume, which means the Volume for the currency\n",
    "    df6.rename(columns={currency: \"volume\"}, inplace=True)\n",
    "    \n",
    "    name=str(currency)\n",
    "\n",
    "    #merging dataframes into a single dataframe\n",
    "    temp_2 = pd.merge(df1, df2, left_on='Date', right_on='Date', how='left')\n",
    "    temp_3 = pd.merge(temp_2, df3, left_on='Date', right_on='Date', how='left')\n",
    "    temp_4 = pd.merge(temp_3, df4, left_on='Date', right_on='Date', how='left')\n",
    "    temp_5 = pd.merge(temp_4, df5, left_on='Date', right_on='Date', how='left')\n",
    "    temp_6 = pd.merge(temp_5, df6, left_on='Date', right_on='Date', how='left')    \n",
    "    df[currency] = temp_6.copy()\n",
    "    df[currency]['Date'] = pd.to_datetime(df[currency]['Date'])\n",
    "    df[currency]['volume'] = df[currency]['volume'].astype('Int64')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e81bd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ts_modeling\">\n",
    "    \n",
    "# 4.0 Time Series Model and Assessment\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd56e42",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"BTC-USD\">\n",
    "    \n",
    "## BTC-USD\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6021b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating a df to predict the crypto currency\n",
    "dfbtc = df['BTC-USD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424ad2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature for better representing day-wise values\n",
    "dfbtc['mean'] = (dfbtc['low'] + dfbtc['high'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24818cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data for any NaN or Null fields\n",
    "dfbtc = dfbtc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17333338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy for applying shift\n",
    "dataset_for_prediction = dfbtc.copy()\n",
    "dataset_for_prediction['Actual']=dataset_for_prediction['close'].shift()\n",
    "dataset_for_prediction=dataset_for_prediction.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8eebaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date time typecast\n",
    "dataset_for_prediction['Date'] =pd.to_datetime(dataset_for_prediction['Date'])\n",
    "dataset_for_prediction.index= dataset_for_prediction['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8088a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the exogeneous variables\n",
    "sc_in = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input = sc_in.fit_transform(dataset_for_prediction[['volume']])  #['low', 'high', 'open', 'adj_close', 'volume', 'mean']\n",
    "scaled_input = pd.DataFrame(scaled_input, index=dataset_for_prediction.index)\n",
    "X=scaled_input\n",
    "X.rename(columns={0:'Volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a7d2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the time series\n",
    "sc_out = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_output = sc_out.fit_transform(dataset_for_prediction[['Actual']])\n",
    "scaler_output =pd.DataFrame(scaler_output, index=dataset_for_prediction.index)\n",
    "y=scaler_output\n",
    "y.rename(columns={0:'Observed Data'}, inplace= True)\n",
    "y.index=dataset_for_prediction.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "156227ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split (cannot shuffle in case of time series)\n",
    "train_X, train_y = X[:-7].dropna(), y[:-7].dropna()\n",
    "test_X, test_y = X[-9:].dropna(), y[-8:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04225146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Init the best SARIMAX model\n",
    "model = SARIMAX(\n",
    "    train_y,\n",
    "    exog=train_X,\n",
    "    order=(0,1,0),\n",
    "    seasonal_order =(2, 1, 0, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d0e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a1de6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2611b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "fcst = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X).to_frame()\n",
    "fcst2 = sc_out.inverse_transform(fcst)\n",
    "#storing the predictions in a dataframe\n",
    "btc_predictions = pd.DataFrame(fcst2, index = fcst.index, columns = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fef58",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ETH-USD\">\n",
    "    \n",
    "## ETH-USD\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec29bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df to predict the crypto currency\n",
    "dfeth = df['ETH-USD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb436d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature for better representing day-wise values\n",
    "dfeth['mean'] = (dfeth['low'] + dfeth['high'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27133223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data for any NaN or Null fields\n",
    "dfeth = dfeth.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6bb3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy for applying shift\n",
    "dataset_for_prediction = dfeth.copy()\n",
    "dataset_for_prediction['Actual']=dataset_for_prediction['close'].shift()\n",
    "dataset_for_prediction=dataset_for_prediction.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9b6bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date time typecast\n",
    "dataset_for_prediction['Date'] =pd.to_datetime(dataset_for_prediction['Date'])\n",
    "dataset_for_prediction.index= dataset_for_prediction['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b792e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the exogeneous variables\n",
    "sc_in = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input = sc_in.fit_transform(dataset_for_prediction[['volume']])  #['low', 'high', 'open', 'adj_close', 'volume', 'mean']\n",
    "scaled_input = pd.DataFrame(scaled_input, index=dataset_for_prediction.index)\n",
    "X=scaled_input\n",
    "X.rename(columns={0:'Volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b007ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the time series\n",
    "sc_out = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_output = sc_out.fit_transform(dataset_for_prediction[['Actual']])\n",
    "scaler_output =pd.DataFrame(scaler_output, index=dataset_for_prediction.index)\n",
    "y=scaler_output\n",
    "y.rename(columns={0:'Observed Data'}, inplace= True)\n",
    "y.index=dataset_for_prediction.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c39ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split (cannot shuffle in case of time series)\n",
    "train_X, train_y = X[:-7].dropna(), y[:-7].dropna()\n",
    "test_X, test_y = X[-9:].dropna(), y[-8:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e564f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Init the best SARIMAX model\n",
    "model = SARIMAX(\n",
    "    train_y,\n",
    "    exog=train_X,\n",
    "    order=(1,1,0),\n",
    "    seasonal_order =(2, 1, 0, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18a90141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "116e5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad050071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "fcst = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X).to_frame()\n",
    "fcst2 = sc_out.inverse_transform(fcst)\n",
    "#storing the predictions in a dataframe\n",
    "eth_predictions = pd.DataFrame(fcst2, index = fcst.index, columns = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a2fdf",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"LINK-USD\">\n",
    "    \n",
    "## LINK-USD\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ac28c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df to predict the crypto currency\n",
    "dflink = df['LINK-USD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90b71922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature for better representing day-wise values\n",
    "dflink['mean'] = (dflink['low'] + dflink['high'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61426472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data for any NaN or Null fields\n",
    "dflink = dflink.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e03a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy for applying shift\n",
    "dataset_for_prediction = dflink.copy()\n",
    "dataset_for_prediction['Actual']=dataset_for_prediction['close'].shift()\n",
    "dataset_for_prediction=dataset_for_prediction.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4224dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date time typecast\n",
    "dataset_for_prediction['Date'] =pd.to_datetime(dataset_for_prediction['Date'])\n",
    "dataset_for_prediction.index= dataset_for_prediction['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e97f8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the exogeneous variables\n",
    "sc_in = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input = sc_in.fit_transform(dataset_for_prediction[['volume']])  #['low', 'high', 'open', 'adj_close', 'volume', 'mean']\n",
    "scaled_input = pd.DataFrame(scaled_input, index=dataset_for_prediction.index)\n",
    "X=scaled_input\n",
    "X.rename(columns={0:'Volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7baf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the time series\n",
    "sc_out = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_output = sc_out.fit_transform(dataset_for_prediction[['Actual']])\n",
    "scaler_output =pd.DataFrame(scaler_output, index=dataset_for_prediction.index)\n",
    "y=scaler_output\n",
    "y.rename(columns={0:'Observed Data'}, inplace= True)\n",
    "y.index=dataset_for_prediction.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71d45958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split (cannot shuffle in case of time series)\n",
    "train_X, train_y = X[:-7].dropna(), y[:-7].dropna()\n",
    "test_X, test_y = X[-9:].dropna(), y[-8:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28d18a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Init the best SARIMAX model\n",
    "model = SARIMAX(\n",
    "    train_y,\n",
    "    exog=train_X,\n",
    "    order=(1,1,1),\n",
    "    seasonal_order =(2, 1, 0, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7df8d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e1b77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6070d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "fcst = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X).to_frame()\n",
    "fcst2 = sc_out.inverse_transform(fcst)\n",
    "#storing the predictions in a dataframe\n",
    "link_predictions = pd.DataFrame(fcst2, index = fcst.index, columns = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189dc292",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"MATIC-USD\">\n",
    "    \n",
    "## MATIC-USD\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e20a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df to predict the crypto currency\n",
    "dfmatic = df['MATIC-USD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e2e2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature for better representing day-wise values\n",
    "dfmatic['mean'] = (dfmatic['low'] + dfmatic['high'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c147971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data for any NaN or Null fields\n",
    "dfmatic = dfmatic.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbfcd9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy for applying shift\n",
    "dataset_for_prediction = dfmatic.copy()\n",
    "dataset_for_prediction['Actual']=dataset_for_prediction['close'].shift()\n",
    "dataset_for_prediction=dataset_for_prediction.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8ae683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date time typecast\n",
    "dataset_for_prediction['Date'] =pd.to_datetime(dataset_for_prediction['Date'])\n",
    "dataset_for_prediction.index= dataset_for_prediction['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99cacc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the exogeneous variables\n",
    "sc_in = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input = sc_in.fit_transform(dataset_for_prediction[['volume']])  #['low', 'high', 'open', 'adj_close', 'volume', 'mean']\n",
    "scaled_input = pd.DataFrame(scaled_input, index=dataset_for_prediction.index)\n",
    "X=scaled_input\n",
    "X.rename(columns={0:'Volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61cb9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the time series\n",
    "sc_out = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_output = sc_out.fit_transform(dataset_for_prediction[['Actual']])\n",
    "scaler_output =pd.DataFrame(scaler_output, index=dataset_for_prediction.index)\n",
    "y=scaler_output\n",
    "y.rename(columns={0:'Observed Data'}, inplace= True)\n",
    "y.index=dataset_for_prediction.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cd94f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split (cannot shuffle in case of time series)\n",
    "train_X, train_y = X[:-7].dropna(), y[:-7].dropna()\n",
    "test_X, test_y = X[-9:].dropna(), y[-8:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "776e285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Init the best SARIMAX model\n",
    "model = SARIMAX(\n",
    "    train_y,\n",
    "    exog=train_X,\n",
    "    order=(2,1,2),\n",
    "    seasonal_order =(2, 1, 0, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b78aaa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a12a2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ef325e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "fcst = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X).to_frame()\n",
    "fcst2 = sc_out.inverse_transform(fcst)\n",
    "#storing the predictions in a dataframe\n",
    "matic_predictions = pd.DataFrame(fcst2, index = fcst.index, columns = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658848c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"SOL-USD\">\n",
    "    \n",
    "## SOL-USD\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6757a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df to predict the crypto currency\n",
    "dfsol = df['SOL-USD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6952daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature for better representing day-wise values\n",
    "dfsol['mean'] = (dfsol['low'] + dfsol['high'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a54aaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data for any NaN or Null fields\n",
    "dfsol = dfsol.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4594c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy for applying shift\n",
    "dataset_for_prediction = dfsol.copy()\n",
    "dataset_for_prediction['Actual']=dataset_for_prediction['close'].shift()\n",
    "dataset_for_prediction=dataset_for_prediction.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "227e19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date time typecast\n",
    "dataset_for_prediction['Date'] =pd.to_datetime(dataset_for_prediction['Date'])\n",
    "dataset_for_prediction.index= dataset_for_prediction['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4a55a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the exogeneous variables\n",
    "sc_in = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input = sc_in.fit_transform(dataset_for_prediction[['volume']])  #['low', 'high', 'open', 'adj_close', 'volume', 'mean']\n",
    "scaled_input = pd.DataFrame(scaled_input, index=dataset_for_prediction.index)\n",
    "X=scaled_input\n",
    "X.rename(columns={0:'Volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "970e8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the time series\n",
    "sc_out = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_output = sc_out.fit_transform(dataset_for_prediction[['Actual']])\n",
    "scaler_output =pd.DataFrame(scaler_output, index=dataset_for_prediction.index)\n",
    "y=scaler_output\n",
    "y.rename(columns={0:'Observed Data'}, inplace= True)\n",
    "y.index=dataset_for_prediction.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e03b60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split (cannot shuffle in case of time series)\n",
    "train_X, train_y = X[:-7].dropna(), y[:-7].dropna()\n",
    "test_X, test_y = X[-9:].dropna(), y[-8:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0b06ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gabriel.souza_ifood\\anaconda3\\envs\\data-mining-env\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Init the best SARIMAX model\n",
    "model = SARIMAX(\n",
    "    train_y,\n",
    "    exog=train_X,\n",
    "    order=(0,1,0),\n",
    "    seasonal_order =(2, 1, 0, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c41a3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe3965bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c46c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "fcst = results.predict(start= len(train_y), end= len(train_y)+len(test_y), exog = test_X).to_frame()\n",
    "fcst2 = sc_out.inverse_transform(fcst)\n",
    "#storing the predictions in a dataframe\n",
    "sol_predictions = pd.DataFrame(fcst2, index = fcst.index, columns = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f82e22",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ts_predictions\">\n",
    "    \n",
    "##  Time Series Predictions Summary\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f19cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating dataframes for each currency to summarize the predictions\n",
    "\n",
    "btc_predictions.index.name = 'Date'\n",
    "btc = btc_predictions.rename(columns={'price': 'BTC-USD'}).reset_index()\n",
    "\n",
    "eth_predictions.index.name = 'Date'\n",
    "eth = eth_predictions.rename(columns={'price': 'ETH-USD'}).reset_index()\n",
    "\n",
    "link_predictions.index.name = 'Date'\n",
    "link = link_predictions.rename(columns={'price': 'LINK-USD'}).reset_index()\n",
    "\n",
    "matic_predictions.index.name = 'Date'\n",
    "matic = matic_predictions.rename(columns={'price': 'MATIC-USD'}).reset_index()\n",
    "\n",
    "sol_predictions.index.name = 'Date'\n",
    "sol = sol_predictions.rename(columns={'price': 'SOL-USD'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ed3aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_a = pd.merge(btc, link, left_on='Date', right_on='Date', how='inner')\n",
    "temp_b = pd.merge(temp_a, matic, left_on='Date', right_on='Date', how='inner')\n",
    "temp_c = pd.merge(temp_b, eth, left_on='Date', right_on='Date', how='inner')\n",
    "final_predictions = pd.merge(temp_c, sol, left_on='Date', right_on='Date', how='inner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "685bc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataframe to store the predictions for D+1 and D+2 of each currency\n",
    "df_pred_final = final_predictions[-2:].copy()\n",
    "\n",
    "#creating the dataframe to store the predictions for the validation data\n",
    "df_val_final = final_predictions[:-2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a30bb2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ml\">\n",
    "    \n",
    "# 5.0 Machine Learning\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4df9ae",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"data_prep_ml\">\n",
    "    \n",
    "# 6.0 Data Preparation and Preprocessing\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e77b38a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed\n"
     ]
    }
   ],
   "source": [
    "#importing data from yahoo finance lib\n",
    "data = yf.download(cryptocurrencies, period = '495d', interval = '1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcbcd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing each indicator in separately dataframe\n",
    "df_open = data['Open'].reset_index()\n",
    "\n",
    "df_close = data['Close'].reset_index()\n",
    "\n",
    "df_adj_close = data['Adj Close'].reset_index()\n",
    "\n",
    "df_high = data['High'].reset_index()\n",
    "\n",
    "df_low = data['Low'].reset_index()\n",
    "\n",
    "df_volume = data['Volume'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "653ee2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "    \n",
    "    df[currency] = pd.DataFrame()\n",
    "\n",
    "    #retrieving open price\n",
    "    df1 = df_open[['Date',currency]].copy()\n",
    "    #filtering only non-null records\n",
    "    df1 = df1[~df1[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to open, which means the Open price for the currency\n",
    "    df1.rename(columns={currency: \"open\"}, inplace=True)\n",
    "\n",
    "    #retrieving close price\n",
    "    df2 = df_close[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df2 = df2[~df2[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to close, which means the Open price for the currency\n",
    "    df2.rename(columns={currency: \"close\"}, inplace=True)\n",
    "\n",
    "    #retrieving adj_close price\n",
    "    df3 = df_adj_close[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df3 = df3[~df3[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to adj_close, which means the adj_close price for the currency\n",
    "    df3.rename(columns={currency: \"adj_close\"}, inplace=True)\n",
    "\n",
    "    #retrieving highest price\n",
    "    df4 = df_high[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df4 = df4[~df4[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to high, which means the highest price for the currency\n",
    "    df4.rename(columns={currency: \"high\"}, inplace=True)\n",
    "\n",
    "    #retrieving lowest price\n",
    "    df5 = df_low[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df5 = df5[~df5[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to df5, which means the lowest price for the currency\n",
    "    df5.rename(columns={currency: \"low\"}, inplace=True)\n",
    "\n",
    "    #retrieving Volume\n",
    "    df6 = df_volume[['Date',currency]]\n",
    "    #filtering only non-null records\n",
    "    df6 = df6[~df6[currency].isnull()].copy()\n",
    "    #renaming column ETH-USD to Volume, which means the Volume for the currency\n",
    "    df6.rename(columns={currency: \"volume\"}, inplace=True)\n",
    "    \n",
    "    name=str(currency)\n",
    "\n",
    "    #merging dataframes into a single dataframe\n",
    "    temp_2 = pd.merge(df1, df2, left_on='Date', right_on='Date', how='left')\n",
    "    temp_3 = pd.merge(temp_2, df3, left_on='Date', right_on='Date', how='left')\n",
    "    temp_4 = pd.merge(temp_3, df4, left_on='Date', right_on='Date', how='left')\n",
    "    temp_5 = pd.merge(temp_4, df5, left_on='Date', right_on='Date', how='left')\n",
    "    temp_6 = pd.merge(temp_5, df6, left_on='Date', right_on='Date', how='left')    \n",
    "    df[currency] = temp_6.copy()\n",
    "    df[currency]['Date'] = pd.to_datetime(df[currency]['Date'])\n",
    "    df[currency]['volume'] = df[currency]['volume'].astype('Int64')\n",
    "    \n",
    "    #Adding Three new rows to the dataset\n",
    "    df[currency] = df[currency].append(\n",
    "        pd.DataFrame({'Date': pd.date_range(start=df[currency].Date.iloc[-1], periods=4, freq='D', closed='right')}))\n",
    "    df[currency].reset_index(inplace=True,drop=True)\n",
    "\n",
    "    #Feature Engineering\n",
    "    df[currency]['year'] = pd.DatetimeIndex(df[currency]['Date']).year\n",
    "    df[currency]['quarter'] = pd.DatetimeIndex(df[currency]['Date']).quarter\n",
    "    df[currency]['month'] = pd.DatetimeIndex(df[currency]['Date']).month\n",
    "    df[currency]['week_number_year'] = pd.DatetimeIndex(df[currency]['Date']).week\n",
    "    df[currency]['day_of_the_week'] = pd.DatetimeIndex(df[currency]['Date']).weekday\n",
    "    df[currency]['day'] = pd.DatetimeIndex(df[currency]['Date']).day "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b121989",
   "metadata": {},
   "source": [
    "#### Creating the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8c0b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "for currency in list_of_currencys:    \n",
    "    df[currency]['target_close'] = df[currency]['close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4267b",
   "metadata": {},
   "source": [
    "#### Shifting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9ea8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for currency in list_of_currencys:\n",
    "    df_temp_shift = df[currency][['open', 'close', 'adj_close', 'high', 'low', 'volume']].shift(+3)\n",
    "    df_temp_target = df[currency][['Date','year','quarter','month','week_number_year','day_of_the_week','day','target_close']]\n",
    "    df_temp_final = pd.concat([df_temp_shift,df_temp_target], axis = 1)\n",
    "    #df[currency] = df_temp_final\n",
    "    df[currency] = df_temp_final.iloc[3:, :]\n",
    "    df[currency].reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0f3ab",
   "metadata": {},
   "source": [
    "#### Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fd13777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserving the original datasets\n",
    "df_original = {}\n",
    "for currency in list_of_currencys:    \n",
    "    df_original[currency] = df[currency].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebf07bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Technical Analysis Features\n",
    "for currency in list_of_currencys:    \n",
    "    ta.add_all_ta_features(df[currency], \"open\", \"high\", \"low\", \"close\", \"volume\", fillna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c5a7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the features trend_psar_up and trend_psar_down due to the quantity of NaN values\n",
    "for currency in list_of_currencys:    \n",
    "    df[currency].drop(['trend_psar_up','trend_psar_down'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5344f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the types of volume_adi and volume_obv from object to float64\n",
    "for currency in list_of_currencys:    \n",
    "    df[currency] = df[currency].astype({'volume_adi':'float64','volume_obv':'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "faafc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing NaN values from the new features\n",
    "\n",
    "list_columns_to_drop_NaN = df['ADA-USD'].columns.tolist()\n",
    "list_columns_to_drop_NaN.remove('target_close')\n",
    "\n",
    "for currency in list_of_currencys:    \n",
    "    df[currency].dropna(axis=0, how='any', subset=list_columns_to_drop_NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fc5b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting indexes and Checking new features\n",
    "for currency in list_of_currencys:\n",
    "    df[currency].reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27395a20",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4cc15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copies of the datasets for the purpose of feature selection\n",
    "df_FS = {}\n",
    "for currency in list_of_currencys:    \n",
    "    df_FS[currency] = df[currency].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca400c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Missing Values in the target (last three rows - new dates to be predicted)\n",
    "for currency in list_of_currencys:    \n",
    "    df_FS[currency].dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "298d3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the date interval - size of the series\n",
    "date_interval = (df_FS['BTC-USD']['Date'].max() - df_FS['BTC-USD']['Date'].min()) / np.timedelta64(1,'D')\n",
    "\n",
    "#measuring the split size\n",
    "split_size = date_interval*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5dae44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the split date\n",
    "split_date = df_FS['BTC-USD']['Date'].min() + datetime.timedelta(days=split_size)\n",
    "split_date = split_date.strftime('%Y-%m-%d')\n",
    "\n",
    "#calculates the initial date\n",
    "init_date = df_FS['SOL-USD']['Date'].min().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4649eb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA-USD  |  Optimum number of features: 1\n",
      "ADA-USD  |  Score with 1 features: 0.422873 \n",
      "\n",
      "ATOM-USD  |  Optimum number of features: 3\n",
      "ATOM-USD  |  Score with 3 features: 0.791099 \n",
      "\n",
      "AVAX-USD  |  Optimum number of features: 10\n",
      "AVAX-USD  |  Score with 10 features: 0.813130 \n",
      "\n",
      "AXS-USD  |  Optimum number of features: 7\n",
      "AXS-USD  |  Score with 7 features: 0.824699 \n",
      "\n",
      "BTC-USD  |  Optimum number of features: 9\n",
      "BTC-USD  |  Score with 9 features: 0.711438 \n",
      "\n",
      "ETH-USD  |  Optimum number of features: 1\n",
      "ETH-USD  |  Score with 1 features: 0.774560 \n",
      "\n",
      "LINK-USD  |  Optimum number of features: 5\n",
      "LINK-USD  |  Score with 5 features: 0.195152 \n",
      "\n",
      "LUNA1-USD  |  Optimum number of features: 1\n",
      "LUNA1-USD  |  Score with 1 features: 0.857307 \n",
      "\n",
      "MATIC-USD  |  Optimum number of features: 3\n",
      "MATIC-USD  |  Score with 3 features: 0.706565 \n",
      "\n",
      "SOL-USD  |  Optimum number of features: 3\n",
      "SOL-USD  |  Score with 3 features: 0.680714 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection using RFE and LinearRegression as the model\n",
    "\n",
    "#Dataset splitting parameters\n",
    "initial_Date = init_date #the minimum date in common to all the criptocurrencies\n",
    "splitting_Date = split_date #70% for training and 30% for validation\n",
    "\n",
    "selected_features = {}\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X = df_FS[currency].drop(['Date','target_close'], axis = 1)\n",
    "    y = df_FS[currency]['target_close']\n",
    "    initial_index = df_FS[currency].index[df_FS[currency]['Date'] == initial_Date].tolist()\n",
    "    splitt_index = df_FS[currency].index[df_FS[currency]['Date'] == splitting_Date].tolist()    \n",
    "    X_train = X[initial_index[0]:splitt_index[0]]\n",
    "    X_val = X[splitt_index[0]:-3]\n",
    "    y_train = y[initial_index[0]:splitt_index[0]]\n",
    "    y_val = y[splitt_index[0]:-3]\n",
    "    \n",
    "    # Scalling the data using MinMaxScaler to scale between 1 and 0\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns).set_index(X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled, columns = X_val.columns).set_index(X_val.index)\n",
    "      \n",
    "    # Checking the ideal number of features\n",
    "    nof_list=np.arange(1,len(X_train_scaled.columns))\n",
    "    high_score=0\n",
    "    nof=0           \n",
    "    score_list =[]\n",
    "    for n in range(len(nof_list)):\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model,nof_list[n])\n",
    "        X_train_rfe = rfe.fit_transform(X_train_scaled,y_train)\n",
    "        X_val_rfe = rfe.transform(X_val_scaled)\n",
    "        model.fit(X_train_rfe,y_train)\n",
    "\n",
    "        score = model.score(X_val_rfe,y_val)\n",
    "        score_list.append(score)\n",
    "\n",
    "        if(score>high_score):\n",
    "            high_score = score\n",
    "            nof = nof_list[n]\n",
    "    print(currency,' | ',\"Optimum number of features: %d\" %nof)\n",
    "    print(currency,' | ',\"Score with %d features: %f\" % (nof, high_score),'\\n')\n",
    "    \n",
    "    # Recursive Feature Elimination (RFE)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(estimator = model, n_features_to_select = nof)\n",
    "    X_rfe = rfe.fit_transform(X = X_train_scaled, y = y_train)\n",
    "    temp_series = pd.Series(rfe.support_, index = X_train_scaled.columns)\n",
    "    selected_features[currency] = temp_series[temp_series].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01916a1a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ml_modeling\">\n",
    "    \n",
    "# 7.0 Machine Learning Model and Assessment\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd915e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"linear\">\n",
    "    \n",
    "## Linear Regression\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3fdb6291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pred = {}\n",
    "summary_LR = pd.DataFrame(index=['New_pred_1','New_pred_2'])\n",
    "\n",
    "#Dataset splitting parameters\n",
    "initial_Date = init_date\n",
    "splitting_Date= split_date\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "    \n",
    "    # List of features to be used\n",
    "    feat_list = []\n",
    "    feat_list = selected_features[currency]\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X = df[currency][feat_list]\n",
    "    y = df[currency]['target_close']\n",
    "    initial_index = df[currency].index[df[currency]['Date'] == initial_Date].tolist()\n",
    "    splitt_index = df[currency].index[df[currency]['Date'] == splitting_Date].tolist()    \n",
    "    X_train = X[initial_index[0]:splitt_index[0]]\n",
    "    X_val = X[splitt_index[0]:-3]\n",
    "    y_train = y[initial_index[0]:splitt_index[0]]\n",
    "    y_val = y[splitt_index[0]:-3]\n",
    "    X_new_pred = X[-3:]\n",
    "    \n",
    "#     # Splitting the dataset - classic approach\n",
    "#     X = df[currency].drop(['Date','target_close'], axis = 1)\n",
    "#     y = df[currency]['target_close']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.1, random_state=0, stratify=None, shuffle=False)\n",
    "\n",
    "    #Training the model and running the predictions\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    model_pred = model.predict(X_val)\n",
    "    model_pred_df = pd.DataFrame(data=model_pred, index=X_val.index, columns=['pred'])\n",
    "    \n",
    "    #Running predictions for the training dataset for overfitting checking purpose\n",
    "    model_pred_train = model.predict(X_train)\n",
    "    model_pred_train_df = pd.DataFrame(data=model_pred_train, index=X_train.index, columns=['pred'])\n",
    "    model_pred_train_df = pd.concat([df[currency],model_pred_train_df], axis = 1)\n",
    "    \n",
    "    # Predicting the new data\n",
    "    model_new_pred = model.predict(X_new_pred)\n",
    "    model_new_pred_df = pd.DataFrame(data=model_new_pred, index=X_new_pred.index, columns=['pred'])\n",
    "    model_new_pred_df = pd.concat([df[currency],model_new_pred_df], axis = 1)\n",
    "    \n",
    "    \n",
    "    #RMSE and R-squared\n",
    "    rmse_val = np.sqrt(MSE(y_val,model_pred))\n",
    "    mape_val = MAPE(y_val,model_pred)\n",
    "    rmse_train = np.sqrt(MSE(y_train,model_pred_train))\n",
    "    mape_train = MAPE(y_train,model_pred_train)\n",
    "\n",
    "    #Concatenating the predictions to the original Dataset\n",
    "    df_pred[currency] = pd.concat([df[currency],model_pred_df], axis = 1)\n",
    "\n",
    "    \n",
    "    summary_LR[currency] = [round(model_new_pred_df[-3:-2]['pred'].tolist()[0],2),\n",
    "                            round(model_new_pred_df[-2:-1]['pred'].tolist()[0],2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7642dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the predictions from Linear Regression\n",
    "lr_predictions = summary_LR[['AVAX-USD','LUNA1-USD']].reset_index(drop=True)\n",
    "\n",
    "#calling the predictions from ARIMA\n",
    "df_arima = df_pred_final.reset_index(drop=True)\n",
    "\n",
    "#storing the predictions in result_a\n",
    "result_a = pd.concat([df_arima, lr_predictions], axis=1)\n",
    "\n",
    "#storing the validation predictions for the chosen currencies\n",
    "lr_avax = df_pred['AVAX-USD'][['Date','target_close']][-10:-3]\n",
    "lr_avax = lr_avax.rename(columns={'target_close': 'AVAX-USD'})      \n",
    "lr_luna = df_pred['LUNA1-USD'][['Date','target_close']][-10:-3]\n",
    "lr_luna = lr_luna.rename(columns={'target_close': 'LUNA1-USD'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd99456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#storing the validation predictions for the chosen currencies\n",
    "temp_val_a = pd.merge(df_val_final, lr_avax, left_on='Date', right_on='Date', how='inner')\n",
    "temp_val_b = pd.merge(temp_val_a, lr_luna, left_on='Date', right_on='Date', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609bf29",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"rand_forest\">\n",
    "    \n",
    "## Random Forest\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b917d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = {}\n",
    "summary_RF = pd.DataFrame(index=['New_pred_1','New_pred_2'])\n",
    "\n",
    "#Dataset splitting parameters\n",
    "initial_Date = init_date\n",
    "splitting_Date= split_date\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "    \n",
    "    # List of features to be used\n",
    "    feat_list = []\n",
    "    feat_list = selected_features[currency]\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X = df[currency][feat_list]\n",
    "    y = df[currency]['target_close']\n",
    "    initial_index = df[currency].index[df[currency]['Date'] == initial_Date].tolist()\n",
    "    splitt_index = df[currency].index[df[currency]['Date'] == splitting_Date].tolist()    \n",
    "    X_train = X[initial_index[0]:splitt_index[0]]\n",
    "    X_val = X[splitt_index[0]:-3]\n",
    "    y_train = y[initial_index[0]:splitt_index[0]]\n",
    "    y_val = y[splitt_index[0]:-3]\n",
    "    X_new_pred = X[-3:]\n",
    "    \n",
    "#     # Splitting the dataset - classic approach\n",
    "#     X = df[currency].drop(['Date','target_close'], axis = 1)\n",
    "#     y = df[currency]['target_close']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.1, random_state=0, stratify=None, shuffle=False)\n",
    "\n",
    "    #Training the model and running the predictions\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    model_pred = model.predict(X_val)\n",
    "    model_pred_df = pd.DataFrame(data=model_pred, index=X_val.index, columns=['pred'])\n",
    "    \n",
    "    #Running predictions for the training dataset for overfitting checking purpose\n",
    "    model_pred_train = model.predict(X_train)\n",
    "    model_pred_train_df = pd.DataFrame(data=model_pred_train, index=X_train.index, columns=['pred'])\n",
    "    model_pred_train_df = pd.concat([df[currency],model_pred_train_df], axis = 1)\n",
    "    \n",
    "    # Predicting the new data\n",
    "    model_new_pred = model.predict(X_new_pred)\n",
    "    model_new_pred_df = pd.DataFrame(data=model_new_pred, index=X_new_pred.index, columns=['pred'])\n",
    "    model_new_pred_df = pd.concat([df[currency],model_new_pred_df], axis = 1)\n",
    "    \n",
    "    \n",
    "    #RMSE and R-squared\n",
    "    rmse_val = np.sqrt(MSE(y_val,model_pred))\n",
    "    mape_val = MAPE(y_val,model_pred)\n",
    "    rmse_train = np.sqrt(MSE(y_train,model_pred_train))\n",
    "    mape_train = MAPE(y_train,model_pred_train)\n",
    "\n",
    "    #Concatenating the predictions to the original Dataset\n",
    "    df_pred[currency] = pd.concat([df[currency],model_pred_df], axis = 1)\n",
    "\n",
    "    summary_RF[currency] = [round(model_new_pred_df[-3:-2]['pred'].tolist()[0],2),\n",
    "                            round(model_new_pred_df[-2:-1]['pred'].tolist()[0],2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "525e9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the predictions from Random Forest\n",
    "rf_predictions = summary_RF['ADA-USD'].reset_index(drop=True)\n",
    "\n",
    "#storing the predictions in result\n",
    "result_b = pd.concat([result_a, rf_predictions], axis=1)\n",
    "\n",
    "#storing the validation predictions for the chosen currencies\n",
    "rf_ada = df_pred['ADA-USD'][['Date','target_close']][-10:-3]\n",
    "rf_ada = rf_ada.rename(columns={'target_close': 'ADA-USD'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17037bd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"svm\">\n",
    "    \n",
    "## Support Vector Machines\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68c2e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = {}\n",
    "summary_SVM = pd.DataFrame(index=['New_pred_1','New_pred_2'])\n",
    "\n",
    "#Dataset splitting parameters\n",
    "initial_Date = init_date\n",
    "splitting_Date= split_date\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "    \n",
    "    # List of features to be used\n",
    "    feat_list = []\n",
    "    feat_list = selected_features[currency]\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X = df[currency][feat_list]\n",
    "    y = df[currency]['target_close']\n",
    "    initial_index = df[currency].index[df[currency]['Date'] == initial_Date].tolist()\n",
    "    splitt_index = df[currency].index[df[currency]['Date'] == splitting_Date].tolist()    \n",
    "    X_train = X[initial_index[0]:splitt_index[0]]\n",
    "    X_val = X[splitt_index[0]:-3]\n",
    "    y_train = y[initial_index[0]:splitt_index[0]]\n",
    "    y_val = y[splitt_index[0]:-3]\n",
    "    X_new_pred = X[-3:]\n",
    "    \n",
    "#     # Splitting the dataset - classic approach\n",
    "#     X = df[currency].drop(['Date','target_close'], axis = 1)\n",
    "#     y = df[currency]['target_close']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.1, random_state=0, stratify=None, shuffle=False)\n",
    "\n",
    "    #Training the model and running the predictions\n",
    "    model = svm.SVR()\n",
    "    model.fit(X_train,y_train)\n",
    "    model_pred = model.predict(X_val)\n",
    "    model_pred_df = pd.DataFrame(data=model_pred, index=X_val.index, columns=['pred'])\n",
    "    \n",
    "    #Running predictions for the training dataset for overfitting checking purpose\n",
    "    model_pred_train = model.predict(X_train)\n",
    "    model_pred_train_df = pd.DataFrame(data=model_pred_train, index=X_train.index, columns=['pred'])\n",
    "    model_pred_train_df = pd.concat([df[currency],model_pred_train_df], axis = 1)\n",
    "    \n",
    "    # Predicting the new data\n",
    "    model_new_pred = model.predict(X_new_pred)\n",
    "    model_new_pred_df = pd.DataFrame(data=model_new_pred, index=X_new_pred.index, columns=['pred'])\n",
    "    model_new_pred_df = pd.concat([df[currency],model_new_pred_df], axis = 1)\n",
    "    \n",
    "    \n",
    "    #RMSE and R-squared\n",
    "    rmse_val = np.sqrt(MSE(y_val,model_pred))\n",
    "    mape_val = MAPE(y_val,model_pred)\n",
    "    rmse_train = np.sqrt(MSE(y_train,model_pred_train))\n",
    "    mape_train = MAPE(y_train,model_pred_train)\n",
    "\n",
    "    #Concatenating the predictions to the original Dataset\n",
    "    df_pred[currency] = pd.concat([df[currency],model_pred_df], axis = 1)\n",
    "\n",
    "    summary_SVM[currency] = [round(model_new_pred_df[-3:-2]['pred'].tolist()[0],2),\n",
    "                            round(model_new_pred_df[-2:-1]['pred'].tolist()[0],2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "15633ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the predictions from Random Forest\n",
    "svm_predictions = summary_SVM['ATOM-USD'].reset_index(drop=True)\n",
    "\n",
    "#storing the predictions in result\n",
    "result_c = pd.concat([result_b, svm_predictions], axis=1)\n",
    "\n",
    "#storing the validation predictions for the chosen currencies\n",
    "svm_atom = df_pred['ATOM-USD'][['Date','target_close']][-10:-3]\n",
    "svm_atom = svm_atom.rename(columns={'target_close': 'ATOM-USD'})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ff0e5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"neural\">\n",
    "    \n",
    "## Neural Network Regressor\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe17712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = {}\n",
    "summary_NNR = pd.DataFrame(index=['New_pred_1','New_pred_2'])\n",
    "\n",
    "#Dataset splitting parameters\n",
    "initial_Date = init_date\n",
    "splitting_Date= split_date\n",
    "\n",
    "for currency in list_of_currencys:\n",
    "    \n",
    "    # List of features to be used\n",
    "    feat_list = []\n",
    "    feat_list = selected_features[currency]\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X = df[currency][feat_list]\n",
    "    y = df[currency]['target_close']\n",
    "    initial_index = df[currency].index[df[currency]['Date'] == initial_Date].tolist()\n",
    "    splitt_index = df[currency].index[df[currency]['Date'] == splitting_Date].tolist()    \n",
    "    X_train = X[initial_index[0]:splitt_index[0]]\n",
    "    X_val = X[splitt_index[0]:-3]\n",
    "    y_train = y[initial_index[0]:splitt_index[0]]\n",
    "    y_val = y[splitt_index[0]:-3]\n",
    "    X_new_pred = X[-3:]\n",
    "    \n",
    "#     # Splitting the dataset - classic approach\n",
    "#     X = df[currency].drop(['Date','target_close'], axis = 1)\n",
    "#     y = df[currency]['target_close']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.1, random_state=0, stratify=None, shuffle=False)\n",
    "\n",
    "    #Training the model and running the predictions\n",
    "    model = MLPRegressor(hidden_layer_sizes=(100,100,100,100,100,100),max_iter=100,learning_rate_init=0.0001,learning_rate='adaptive')\n",
    "    model.fit(X_train,y_train)\n",
    "    model_pred = model.predict(X_val)\n",
    "    model_pred_df = pd.DataFrame(data=model_pred, index=X_val.index, columns=['pred'])\n",
    "    \n",
    "    #Running predictions for the training dataset for overfitting checking purpose\n",
    "    model_pred_train = model.predict(X_train)\n",
    "    model_pred_train_df = pd.DataFrame(data=model_pred_train, index=X_train.index, columns=['pred'])\n",
    "    model_pred_train_df = pd.concat([df[currency],model_pred_train_df], axis = 1)\n",
    "    \n",
    "    # Predicting the new data\n",
    "    model_new_pred = model.predict(X_new_pred)\n",
    "    model_new_pred_df = pd.DataFrame(data=model_new_pred, index=X_new_pred.index, columns=['pred'])\n",
    "    model_new_pred_df = pd.concat([df[currency],model_new_pred_df], axis = 1)\n",
    "    \n",
    "    \n",
    "    #RMSE and R-squared\n",
    "    rmse_val = np.sqrt(MSE(y_val,model_pred))\n",
    "    mape_val = MAPE(y_val,model_pred)\n",
    "    rmse_train = np.sqrt(MSE(y_train,model_pred_train))\n",
    "    mape_train = MAPE(y_train,model_pred_train)\n",
    "\n",
    "    #Concatenating the predictions to the original Dataset\n",
    "    df_pred[currency] = pd.concat([df[currency],model_pred_df], axis = 1)\n",
    "\n",
    "    summary_NNR[currency] = [round(model_new_pred_df[-3:-2]['pred'].tolist()[0],2),\n",
    "                            round(model_new_pred_df[-2:-1]['pred'].tolist()[0],2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fad955d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the predictions from Random Forest\n",
    "nnr_predictions = summary_NNR['AXS-USD'].reset_index(drop=True)\n",
    "\n",
    "#storing the predictions for D+1 and D+2 in the dataframe df_pred_dashboard\n",
    "df_pred_dashboard = pd.concat([result_c, nnr_predictions], axis=1)\n",
    "\n",
    "#storing the validation predictions for the chosen currencies\n",
    "nnr_axs = df_pred['AXS-USD'][['Date','target_close']][-10:-3]\n",
    "nnr_axs = nnr_axs.rename(columns={'target_close': 'AXS-USD'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbbfc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the validation predictions for the chosen currencies\n",
    "temp_val_a = pd.merge(df_val_final, lr_avax, left_on='Date', right_on='Date', how='inner')\n",
    "temp_val_b = pd.merge(temp_val_a, lr_luna, left_on='Date', right_on='Date', how='inner')\n",
    "temp_val_c = pd.merge(temp_val_b, rf_ada, left_on='Date', right_on='Date', how='inner')\n",
    "temp_val_d = pd.merge(temp_val_c, svm_atom, left_on='Date', right_on='Date', how='inner')\n",
    "df_val_dashboard = pd.merge(temp_val_d, nnr_axs, left_on='Date', right_on='Date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a9f2983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BTC-USD</th>\n",
       "      <th>LINK-USD</th>\n",
       "      <th>MATIC-USD</th>\n",
       "      <th>ETH-USD</th>\n",
       "      <th>SOL-USD</th>\n",
       "      <th>AVAX-USD</th>\n",
       "      <th>LUNA1-USD</th>\n",
       "      <th>ADA-USD</th>\n",
       "      <th>ATOM-USD</th>\n",
       "      <th>AXS-USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>29788.937088</td>\n",
       "      <td>7.202550</td>\n",
       "      <td>0.684968</td>\n",
       "      <td>2010.718204</td>\n",
       "      <td>51.183741</td>\n",
       "      <td>29.026232</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>11.132798</td>\n",
       "      <td>20.923748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>28277.592449</td>\n",
       "      <td>6.425694</td>\n",
       "      <td>0.583550</td>\n",
       "      <td>1888.179571</td>\n",
       "      <td>46.129848</td>\n",
       "      <td>29.025648</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.521224</td>\n",
       "      <td>11.082132</td>\n",
       "      <td>21.247337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>27738.028119</td>\n",
       "      <td>6.074141</td>\n",
       "      <td>0.520240</td>\n",
       "      <td>1777.391835</td>\n",
       "      <td>41.076308</td>\n",
       "      <td>27.225376</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.513877</td>\n",
       "      <td>10.739818</td>\n",
       "      <td>20.898247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>28126.591006</td>\n",
       "      <td>6.205845</td>\n",
       "      <td>0.545342</td>\n",
       "      <td>1810.634183</td>\n",
       "      <td>42.067370</td>\n",
       "      <td>23.550030</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.480883</td>\n",
       "      <td>9.643618</td>\n",
       "      <td>19.561852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>27493.349380</td>\n",
       "      <td>6.000148</td>\n",
       "      <td>0.533776</td>\n",
       "      <td>1764.054651</td>\n",
       "      <td>40.430350</td>\n",
       "      <td>22.418623</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.455507</td>\n",
       "      <td>9.252514</td>\n",
       "      <td>18.198242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>26706.931350</td>\n",
       "      <td>5.652260</td>\n",
       "      <td>0.510146</td>\n",
       "      <td>1710.048201</td>\n",
       "      <td>38.304410</td>\n",
       "      <td>22.749695</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.455911</td>\n",
       "      <td>9.414568</td>\n",
       "      <td>18.242544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>26604.632945</td>\n",
       "      <td>5.634501</td>\n",
       "      <td>0.522695</td>\n",
       "      <td>1716.712163</td>\n",
       "      <td>38.648659</td>\n",
       "      <td>26.166412</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.480782</td>\n",
       "      <td>9.616521</td>\n",
       "      <td>18.423071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       BTC-USD  LINK-USD  MATIC-USD      ETH-USD    SOL-USD  \\\n",
       "0 2022-05-23  29788.937088  7.202550   0.684968  2010.718204  51.183741   \n",
       "1 2022-05-24  28277.592449  6.425694   0.583550  1888.179571  46.129848   \n",
       "2 2022-05-25  27738.028119  6.074141   0.520240  1777.391835  41.076308   \n",
       "3 2022-05-26  28126.591006  6.205845   0.545342  1810.634183  42.067370   \n",
       "4 2022-05-27  27493.349380  6.000148   0.533776  1764.054651  40.430350   \n",
       "5 2022-05-28  26706.931350  5.652260   0.510146  1710.048201  38.304410   \n",
       "6 2022-05-29  26604.632945  5.634501   0.522695  1716.712163  38.648659   \n",
       "\n",
       "    AVAX-USD  LUNA1-USD   ADA-USD   ATOM-USD    AXS-USD  \n",
       "0  29.026232   0.000160  0.512605  11.132798  20.923748  \n",
       "1  29.025648   0.000177  0.521224  11.082132  21.247337  \n",
       "2  27.225376   0.000182  0.513877  10.739818  20.898247  \n",
       "3  23.550030   0.000139  0.480883   9.643618  19.561852  \n",
       "4  22.418623   0.000119  0.455507   9.252514  18.198242  \n",
       "5  22.749695   0.000121  0.455911   9.414568  18.242544  \n",
       "6  26.166412   0.000105  0.480782   9.616521  18.423071  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final validation dataset to be used on Dashboard\n",
    "df_val_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbf2629a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BTC-USD</th>\n",
       "      <th>LINK-USD</th>\n",
       "      <th>MATIC-USD</th>\n",
       "      <th>ETH-USD</th>\n",
       "      <th>SOL-USD</th>\n",
       "      <th>AVAX-USD</th>\n",
       "      <th>LUNA1-USD</th>\n",
       "      <th>ADA-USD</th>\n",
       "      <th>ATOM-USD</th>\n",
       "      <th>AXS-USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>25541.601371</td>\n",
       "      <td>4.911794</td>\n",
       "      <td>0.438159</td>\n",
       "      <td>1604.931930</td>\n",
       "      <td>31.542039</td>\n",
       "      <td>21.34</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.2</td>\n",
       "      <td>11.97</td>\n",
       "      <td>17.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>24745.639369</td>\n",
       "      <td>4.382160</td>\n",
       "      <td>0.364323</td>\n",
       "      <td>1471.716503</td>\n",
       "      <td>25.628087</td>\n",
       "      <td>21.42</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.2</td>\n",
       "      <td>11.89</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       BTC-USD  LINK-USD  MATIC-USD      ETH-USD    SOL-USD  \\\n",
       "0 2022-05-30  25541.601371  4.911794   0.438159  1604.931930  31.542039   \n",
       "1 2022-05-31  24745.639369  4.382160   0.364323  1471.716503  25.628087   \n",
       "\n",
       "   AVAX-USD  LUNA1-USD  ADA-USD  ATOM-USD  AXS-USD  \n",
       "0     21.34       0.92      1.2     11.97    17.89  \n",
       "1     21.42       0.92      1.2     11.89    17.93  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final predictions dataset to be used on Dashboard\n",
    "df_pred_dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028333e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "__FIM__ <br>\n",
    "     \n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
